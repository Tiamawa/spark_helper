<!DOCTYPE html >
<html>
        <head>
          <title>spark_helper - com.spark_helper</title>
          <meta name="description" content="spark helper - com.spark helper" />
          <meta name="keywords" content="spark helper com.spark helper" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../../lib/template.js"></script>
      <script type="text/javascript" src="../../lib/tools.tooltip.js"></script>
      
      <script type="text/javascript">
         if(top === self) {
            var url = '../../index.html';
            var hash = 'com.spark_helper.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../../lib/package_big.png" />
        <p id="owner"><a href="../package.html" class="extype" name="com">com</a></p>
        <h1>spark_helper</h1><span class="permalink">
      <a href="../../index.html#com.spark_helper.package" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">spark_helper</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        
        
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="com.spark_helper.DateHelper" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DateHelper"></a>
      <a id="DateHelper:DateHelper"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="DateHelper$.html"><span class="name">DateHelper</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#com.spark_helper.package@DateHelper" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A facility which deals with usual date needs (wrapper around
<a href="http://www.joda.org/joda-time/apidocs/">joda-time</a>).</a></p><div class="fullcomment"><div class="comment cmt"><p>A facility which deals with usual date needs (wrapper around
<a href="http://www.joda.org/joda-time/apidocs/">joda-time</a>).</p><p>The goal is to remove the maximum of highly used low-level code from your
spark job and replace it with methods fully tested whose name is
self-explanatory/readable.</p><p>A few exemples:</p><pre>assert(DateHelper.daysBetween(<span class="lit">"20161230"</span>, <span class="lit">"20170101"</span>) == <span class="std">List</span>(<span class="lit">"20161230"</span>, <span class="lit">"20161231"</span>, <span class="lit">"20170101"</span>))
assert(DateHelper.today() == <span class="lit">"20170310"</span>) <span class="cmt">// If today's "20170310"</span>
assert(DateHelper.yesterday() == <span class="lit">"20170309"</span>) <span class="cmt">// If today's "20170310"</span>
assert(DateHelper.reformatDate(<span class="lit">"20170327"</span>, <span class="lit">"yyyyMMdd"</span>, <span class="lit">"yyMMdd"</span>) == <span class="lit">"170327"</span>)
assert(DateHelper.now(<span class="lit">"HH:mm"</span>) == <span class="lit">"10:24"</span>)
assert(DateHelper.currentTimestamp() == <span class="lit">"1493105229736"</span>)
assert(DateHelper.nDaysBefore(<span class="num">3</span>) == <span class="lit">"20170307"</span>) <span class="cmt">// If today's "20170310"</span>
assert(DateHelper.nDaysAfterDate(<span class="num">3</span>, <span class="lit">"20170307"</span>) == <span class="lit">"20170310"</span>)</pre><p>Source <a href="https://github.com/xavierguihot/spark_helper/blob/master/src
/main/scala/com/spark_helper/DateHelper.scala">DateHelper</a>
</p></div><dl class="attributes block"> <dt>Since</dt><dd><p>2017-02</p></dd></dl></div>
    </li><li name="com.spark_helper.HdfsHelper" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="HdfsHelper"></a>
      <a id="HdfsHelper:HdfsHelper"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="HdfsHelper$.html"><span class="name">HdfsHelper</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#com.spark_helper.package@HdfsHelper" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A facility to deal with file manipulations (wrapper around hdfs apache
Hadoop FileSystem API <a href="https://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/fs/FileSystem.html">org.apache.hadoop.fs.FileSystem</a>).</a></p><div class="fullcomment"><div class="comment cmt"><p>A facility to deal with file manipulations (wrapper around hdfs apache
Hadoop FileSystem API <a href="https://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/fs/FileSystem.html">org.apache.hadoop.fs.FileSystem</a>).</p><p>The goal is to remove the maximum of highly used low-level code from your
spark job and replace it with methods fully tested whose name is
self-explanatory/readable.</p><p>For instance, one don't want to remove a file from hdfs using 3 lines of
code and thus could instead just use
HdfsHelper.deleteFile(&quot;my/hdfs/file/path.csv&quot;).</p><p>A few exemples:</p><pre><span class="kw">import</span> com.spark_helper.HdfsHelper

<span class="cmt">// A bunch of methods wrapping the FileSystem API, such as:</span>
HdfsHelper.fileExists(<span class="lit">"my/hdfs/file/path.txt"</span>)
assert(HdfsHelper.listFileNamesInFolder(<span class="lit">"my/folder/path"</span>) == <span class="std">List</span>(<span class="lit">"file_name_1.txt"</span>, <span class="lit">"file_name_2.csv"</span>))
assert(HdfsHelper.fileModificationDate(<span class="lit">"my/hdfs/file/path.txt"</span>) == <span class="lit">"20170306"</span>)
assert(HdfsHelper.nbrOfDaysSinceFileWasLastModified(<span class="lit">"my/hdfs/file/path.txt"</span>) == <span class="num">3</span>)
HdfsHelper.deleteFile(<span class="lit">"my/hdfs/file/path.csv"</span>)
HdfsHelper.moveFolder(<span class="lit">"my/hdfs/folder"</span>)
HdfsHelper.compressFile(<span class="lit">"hdfs/path/to/uncompressed_file.txt"</span>, classOf[GzipCodec])
HdfsHelper.appendHeader(<span class="lit">"my/hdfs/file/path.csv"</span>, <span class="lit">"colum0,column1"</span>)

<span class="cmt">// Some Xml/Typesafe helpers for hadoop as well:</span>
HdfsHelper.isHdfsXmlCompliantWithXsd(
  <span class="lit">"my/hdfs/file/path.xml"</span>, getClass.getResource(<span class="lit">"/some_xml.xsd"</span>))
HdfsHelper.loadXmlFileFromHdfs(<span class="lit">"my/hdfs/file/path.xml"</span>)

<span class="cmt">// Very handy to load a config (typesafe format) stored on hdfs at the</span>
<span class="cmt">// begining of a spark job:</span>
HdfsHelper.loadTypesafeConfigFromHdfs(<span class="lit">"my/hdfs/file/path.conf"</span>): Config

<span class="cmt">// In order to write small amount of data in a file on hdfs without the</span>
<span class="cmt">// whole spark stack:</span>
HdfsHelper.writeToHdfsFile(
  <span class="std">Array</span>(<span class="lit">"some"</span>, <span class="lit">"relatively small"</span>, <span class="lit">"text"</span>),
  <span class="lit">"/some/hdfs/file/path.txt"</span>)

<span class="cmt">// Deletes all files/folders in "hdfs/path/to/folder" for which the</span>
<span class="cmt">// timestamp is older than 10 days:</span>
HdfsHelper.purgeFolder(<span class="lit">"hdfs/path/to/folder"</span>, <span class="num">10</span>)</pre><p>Source <a href="https://github.com/xavierguihot/spark_helper/blob/master/src
/main/scala/com/spark_helper/HdfsHelper.scala">HdfsHelper</a>
</p></div><dl class="attributes block"> <dt>Since</dt><dd><p>2017-02</p></dd></dl></div>
    </li><li name="com.spark_helper.Monitor" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Monitor"></a>
      <a id="Monitor:Monitor"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="Monitor$.html"><span class="name">Monitor</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#com.spark_helper.package@Monitor" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A logger dedicated to Spak jobs.</p><div class="fullcomment"><div class="comment cmt"><p>A logger dedicated to Spak jobs.</p><p>It's a simple logger/report which contains a report that one can update from
the driver and a success state. The idea is to persist job executions logs
and errors (and forget about grepping unreadable yarn logs).</p><p>It's designed for perdiodic spark jobs (handles storage and purge of logs)
and provides a way to handle kpis validation.</p><p>Logs are stored on the go which means one can have a direct real time access
of the job logs/status and it's current state (which can overwise be a pain
if it means going through yarn logs, or even for certain production
environments going through additional layers of software logs to get to yarn
logs).</p><p>One of the issues this logger aims at tackling is the handling of exceptions
reported from executors. An exception within a Spark pipeline doesn't always
mean one want to make the job fail. Or if it's the case, one might still
want to perform a few actions before letting the job crash. The idea is thus
to surround (driver side) a Spark pipeline within a try catch and redirect
the exception to the logger for a clean logging.</p><p>This is a &quot;driver-only&quot; logger and is not intended at logging concurrent
actions from executors.</p><p>Produced reports can easily be inserted in a notification email whenerver
the job fails, which saves a lot of time to maintainers operating on heavy
production environements.</p><p>The produced persisted report is also a way for downstream jobs to know the
status of their input data.</p><p>Let's go through a simple Spark job example monitored with this Monitor
facility:</p><pre>Monitor.setTitle(<span class="lit">"My job title"</span>)
Monitor.addDescription(
  <span class="lit">"My job description (whatever you want); for instance:\n"</span> +
  <span class="lit">"Documentation: https://github.com/xavierguihot/spark_helper"</span>)
Monitor.setLogFolder(<span class="lit">"path/to/log/folder"</span>)

<span class="kw">try</span> {

  <span class="cmt">// Let's perform a spark pipeline which might go wrong:</span>
  <span class="kw">val</span> processedData = sc.textFile(<span class="lit">"file.json"</span>).map(<span class="cmt">/**whatever*/</span>)

  <span class="cmt">// Let's say you want to get some KPIs on your output before storing it:</span>
  <span class="kw">val</span> outputIsValid = Monitor.kpis(
    <span class="std">List</span>(
      Test(<span class="lit">"Nbr of output records"</span>, processedData.count(), SUPERIOR_THAN, <span class="num">10</span>e6d, NBR),
      Test(<span class="lit">"Some pct of invalid output"</span>, your_complex_kpi, INFERIOR_THAN, <span class="num">3</span>, PCT)
    ),
    <span class="lit">"My pipeline descirption"</span>
  )

  <span class="kw">if</span> (outputIsValid)
    processedData.saveAsTextFile(<span class="lit">"wherever.csv"</span>)

} <span class="kw">catch</span> {
  <span class="kw">case</span> iie: InvalidInputException <span class="kw">=&gt;</span>
    Monitor.error(iie, <span class="lit">"My pipeline descirption"</span>, diagnostic = <span class="lit">"No input data!"</span>)
  <span class="kw">case</span> e: Throwable <span class="kw">=&gt;</span>
    Monitor.error(e, <span class="lit">"My pipeline descirption"</span>) <span class="cmt">// whatever unexpected error</span>
}

<span class="kw">if</span> (Monitor.isSuccess()) {
  <span class="kw">val</span> doMore = <span class="lit">"Let's do some more stuff!"</span>
  Monitor.log(<span class="lit">"My second pipeline description: success"</span>)
}

<span class="cmt">// At the end of the different steps of the job, we can store the report in</span>
<span class="cmt">// HDFS (this saves the logs in the folder set with Monitor.setLogFolder):</span>
Monitor.store()

<span class="cmt">// At the end of the job, if the job isn't successfull, you might want to</span>
<span class="cmt">// crash it (for instance to get a notification from your scheduler):</span>
<span class="kw">if</span> (!Monitor.isSuccess()) <span class="kw">throw</span> <span class="kw">new</span> Exception() <span class="cmt">// or send an email, or ...</span></pre><p>At any time during the job, logs can be accessed from file
path/to/log/folder/current.ongoing</p><p>If we were to read the stored report after this simple pipeline, here are
some possible reports:</p><p>First scenario, problem with the input of the job:</p><pre>          My job title

My job description (whatever you want); <span class="kw">for</span> instance:
Documentation: https:<span class="cmt">//github.com/xavierguihot/spark_helper</span>
[<span class="num">10</span>:<span class="num">23</span>] Begining
[<span class="num">10</span>:<span class="num">23</span>-<span class="num">10</span>:<span class="num">23</span>] My pipeline descirption: failed
  Diagnostic: No input data!
    org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:<span class="cmt">//my/hdfs/input/path</span>
    at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:<span class="num">285</span>)
    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:<span class="num">228</span>)
    ...
[<span class="num">10</span>:<span class="num">23</span>] Duration: <span class="num">00</span>:<span class="num">00</span>:<span class="num">00</span></pre><p>Another scenario, unexpected problem:</p><pre>          My job title

My job description (whatever you want); <span class="kw">for</span> instance:
Documentation: https:<span class="cmt">//github.com/xavierguihot/spark_helper</span>
[<span class="num">10</span>:<span class="num">23</span>] Begining
[<span class="num">10</span>:<span class="num">23</span>-<span class="num">10</span>:<span class="num">36</span>] My pipeline descirption: failed
    java.lang.NumberFormatException: For input string: <span class="lit">"a"</span>
    java.lang.NumberFormatException.forInputString(NumberFormatException.java:<span class="num">65</span>)
    java.lang.Integer.parseInt(Integer.java:<span class="num">492</span>)
    ...
[<span class="num">10</span>:<span class="num">36</span>] Duration: <span class="num">00</span>:<span class="num">13</span>:<span class="num">47</span></pre><p>Another scenario, successfull spark pipeline and KPIs are valid; all good!:</p><pre>          My job title

My job description (whatever you want); <span class="kw">for</span> instance:
Documentation: https:<span class="cmt">//github.com/xavierguihot/spark_helper</span>
[<span class="num">10</span>:<span class="num">23</span>] Begining
[<span class="num">10</span>:<span class="num">23</span>-<span class="num">10</span>:<span class="num">41</span>] My pipeline descirption: success
  KPI: Nbr of output records
    Value: <span class="num">14669071.0</span>
    Must be superior than <span class="num">10000000.0</span>
    Validated: <span class="kw">true</span>
  KPI: <span class="std">Some</span> pct of invalid output
    Value: <span class="num">0.06</span>%
    Must be inferior than <span class="num">3.0</span>%
    Validated: <span class="kw">true</span>
[<span class="num">10</span>:<span class="num">41</span>-<span class="num">10</span>:<span class="num">42</span>] My second pipeline description: success
[<span class="num">10</span>:<span class="num">42</span>] Duration: <span class="num">00</span>:<span class="num">19</span>:<span class="num">23</span></pre><p>Source <a href="https://github.com/xavierguihot/spark_helper/blob/master/src
/main/scala/com/spark_helper/monitoring/Monitor.scala">Monitor</a>
</p></div><dl class="attributes block"> <dt>Since</dt><dd><p>2017-02</p></dd></dl></div>
    </li><li name="com.spark_helper.SparkHelper" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SparkHelper"></a>
      <a id="SparkHelper:SparkHelper"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="SparkHelper$.html"><span class="name">SparkHelper</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#com.spark_helper.package@SparkHelper" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A facility to deal with RDD/file manipulations based on the Spark API.</p><div class="fullcomment"><div class="comment cmt"><p>A facility to deal with RDD/file manipulations based on the Spark API.</p><p>The goal is to remove the maximum of highly used low-level code from your
spark job and replace it with methods fully tested whose name is
self-explanatory/readable.</p><p>A few exemples:</p><pre><span class="cmt">// Same as SparkContext.saveAsTextFile, but the result is a single file:</span>
SparkHelper.saveAsSingleTextFile(myOutputRDD, <span class="lit">"/my/output/file/path.txt"</span>)
<span class="cmt">// Same as SparkContext.textFile, but instead of reading one record per</span>
<span class="cmt">// line, it reads records spread over several lines.</span>
<span class="cmt">// This way, xml, json, yml or any multi-line record file format can be used</span>
<span class="cmt">// with Spark:</span>
SparkHelper.textFileWithDelimiter(<span class="lit">"/my/input/folder/path"</span>, sparkContext, <span class="lit">"---\n"</span>)
<span class="cmt">// Same as SparkContext.textFile, but instead of returning an RDD of</span>
<span class="cmt">// records, it returns an RDD of tuples containing both the record and the</span>
<span class="cmt">// path of the file it comes from:</span>
SparkHelper.textFileWithFileName(<span class="lit">"folder"</span>, sparkContext)</pre><p>Source <a href="https://github.com/xavierguihot/spark_helper/blob/master/src
/main/scala/com/spark_helper/SparkHelper.scala">SparkHelper</a>
</p></div><dl class="attributes block"> <dt>Since</dt><dd><p>2017-02</p></dd></dl></div>
    </li><li name="com.spark_helper.monitoring" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="monitoring"></a>
      <a id="monitoring:monitoring"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="monitoring/package.html"><span class="name">monitoring</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#com.spark_helper.package@monitoring" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>
